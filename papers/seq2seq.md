# [Sequence to Sequence Learning with Neural Networks](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)


# Summary
This paper presents a deep architecture for generating target sequences of variable length. The architecture uses two deep LSTMs - an encoder LSTM to map the input sequence to a vector of fixed dimensionality (the last hidden state of the encoder LSTM) and a decoder LSTM to generate the output sequence from the vector. The model is then applied to an English to French Machine Translation (MT) task on the publicly available WMT'14 dataset. One of the "tricks" used in the paper is to reverse the order of words in the source sentences which leads to a considerable improvement in performance. Results show that the model is also successful in translating long sequences. It achieves a BLEU score of 34.81 by itself as an end-to-end system (outperforming the 33.3 SMT baseline) and a score of 36.5 when coupled with the baseline system (by re-scoring the 1000 best lists generated by the baseline).


# Highlights
* Reversing the order of words in the source sentences offers a surprising improvement in performance. This is claimed to have helped by introducing many short term dependencies, which in turn help SGD to "establish communication" between the source and target sequences.
* The model is able to translate long sentences (attributed to the reversal of word order).
* Deep LSTMs perform better than shallow LSTMs.
* The 2-D PCA projections of the hidden state are sensitive to the order of words but not to the active/passive voice of the sentence.
* 2x speed-up in training time by grouping together sentences of similar length in the mini-batch.
* Training time is ~10 days on an 8 GPU machine.


# Questions
* The explanation of why reversing the order of words in the source sentences leads to significant boost in performance feels incomplete (as is also pointed out in the paper).
* What does the performance look like when input tokens are reversed in the architecture proposed by [Cho et al.](https://arxiv.org/abs/1406.1078) (34.54 BLEU), which uses a similar architecture but with RNN units instead of LSTM. Can the difference in performance be attributed mainly to inverting the sequences?
 